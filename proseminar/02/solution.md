# PS 2

## Task1 

#### Write a sequential application pi_seq in C or C++ that computes Ï€ for a given number of samples (command line argument). Test your application for various, large sample sizes to verify the correctness of your implementation.


| Samples | Result
|---|---|
|10 |2.40000|
|100 |3.04000|
|1000 |3.21200|
|10000 |3.15840|
|100000 |3.15040|
|1000000 |3.14464|
|10000000 |3.14209|
|100000000 |3.14150|
|1000000000 | 3.14159|

![Plot](pi/pi_seq_plot.png)


#### Consider a parallelization strategy using MPI. Which communication pattern(s) would you choose and why?
We split up the number of samples by the number of slots. Then each slot can compute a part of the problem on its own.
After each slot has computed its number of samples, it will send `(slotNumber != 0)` or recieve `(slotNumber == 0)` the result. 
The slot with `slotNumber == 0`, will then finally artihemtic average of all results.

##### Used mpi calls:
- `MPI_Init`: Initialize mpi
- `MPI_Comm_size`: Get the number of slots
- `MPI_Comm_rank`: Get the own slot number 
- `MPI_Recv`: Blocking recieve
- `MPI_Send`: Blocking send
- `MPI_Finalize`: Clean up mpi 

#### Implement your chosen parallelization strategy as a second application pi_mpi. Run it with varying numbers of ranks and sample sizes and verify its correctness by comparing the output to pi_seq.
![Plot](pi/pi_mpi_plot.png)


#### Discuss the effects and implications of your parallelization.

- t0 = Time needed for one slot to compute

Since we can simply split up the work and can compute independently we get speedup of t0/(t0/slotsCount) to calculate the partly solutions. 
Only the overhead of sending the final results from each slots `(slotNumber != 0)` to the slit `(slotNumber == 0)` adds some overhead which can be negelected if the problemSize >> slotsCount.



## Task2

#### Consider a parallelization strategy using MPI. Which communication pattern(s) would you choose and why? Are there additional changes required in the code beyond calling MPI functions? If so, elaborate!
We evenly devided the 1D-Array depending on the number of slots. On each timestep every slot calulates the next iteration and afterwards they exchange their bounding values with their neigbours. For this we used `MPI_Isend` (needed to avoid deadlock) and `MPI_Recv`. At the end of our calculation and additionaly every 1000 steps we used `MPI_Gather` to fetch all information into an array of the last slot to print the current state.

##### Used mpi calls:
- `MPI_Init`: Initialize mpi
- `MPI_Comm_size`: Get the number of slots
- `MPI_Comm_rank`: Get the own slot number 
- `MPI_Recv`: Blocking recieve
- `MPI_ISend`: Non blocking send (if everyone would use blocking send, every slot would wait for the reciver, which is also sending at this moment => deadlock)
- `MPI_Gather`: Recieve the data chunks from eacht slot
- `MPI_Finalize`: Clean up mpi 

#### Run it with varying numbers of ranks and problem sizes and verify its correctness by comparing the output to heat_stencil_1D_seq.
For each tested execution the validaiton succeedes and the resulting output is the same. Since the generated output data is quite large, we decided not to include it into the report.

#### Discuss the effects and implications of your parallelization.
We have the overhead of exchanging the bounding values at every timestep(which is not much data to transmit, but still need to synchronize this information with the other slots).  But nevertheless this overhead can be negelected, if the array size is big enough.
Aditional we must gather all the data into the last slot when we want to print out the current state. For this we use `MPI_Gather` which takes care of the whole exchange.

- t0 = Time needed for one slot to compute

If we increase the numbers of avaiable slots the computation itself would speeds up by t0/(t0/slotsCount), but since with an increasing number of slots, the comunication overhead increases. In the end we have 2*(numberOfSlots-1) border values to exchange for each time step and additionaly the overhead of gathering the hole date to print the current state.
